{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kamie≈Ñ milowy 2\n",
    "Micha≈Ç Bili≈Ñski,  311184"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lekko zmodyfikowany kod z wyk≈Çadu + moje funkcje dla operator√≥w skalarnych relu, identity itp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 21 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MLDatasets, Flux, Statistics\n",
    "using LinearAlgebra\n",
    "using Random\n",
    "Random.seed!(1234); #≈ªeby ≈Çatwiej testowaƒá\n",
    "\n",
    "train_data = MLDatasets.MNIST(split=:train)\n",
    "test_data  = MLDatasets.MNIST(split=:test)\n",
    "\n",
    "\n",
    "dim1, dim2, dim3 = size(train_data.features)\n",
    "x1 = train_data.features\n",
    "yhot = Flux.onehotbatch(train_data.targets, 0:9)\n",
    "\n",
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "struct Constant{T} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "mutable struct Variable <: GraphNode\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    cache :: Any\n",
    "    name :: String\n",
    "    Variable(output; name=\"?\") = new(output, nothing, zero(copy(output)), name)\n",
    "end\n",
    "\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "import Base: show, summary\n",
    "show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ‚î£‚îÅ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ‚îó‚îÅ ‚àá \");  summary(io, x.gradient)\n",
    "end\n",
    "function visit(node::GraphNode, visited, order)\n",
    "    if node ‚àà visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "    \n",
    "function visit(node::Operator, visited, order)\n",
    "    if node ‚àà visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode)\n",
    "    visited = Set()\n",
    "    order = Vector()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end\n",
    "reset!(node::Constant) = nothing\n",
    "reset!(node::Variable) = node.gradient = nothing\n",
    "reset!(node::Operator) = node.gradient = nothing\n",
    "\n",
    "compute!(node::Constant) = nothing\n",
    "compute!(node::Variable) = nothing\n",
    "compute!(node::Operator) =\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end\n",
    "update!(node::Constant, gradient) = nothing\n",
    "update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "\n",
    "function backward!(order::Vector; seed=1.0)\n",
    "    result = last(order)\n",
    "    result.gradient = seed\n",
    "    @assert length(result.output) == 1 \"Gradient is defined only for scalar functions\"\n",
    "    for node in reverse(order)\n",
    "        backward!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "import Base: ^\n",
    "^(x::GraphNode, n::GraphNode) = ScalarOperator(^, x, n)\n",
    "forward(::ScalarOperator{typeof(^)}, x, n) = return x^n\n",
    "backward(::ScalarOperator{typeof(^)}, x, n, g) = tuple(g * n * x ^ (n-1), g * log(abs(x)) * x ^ n)\n",
    "import Base: sin\n",
    "sin(x::GraphNode) = ScalarOperator(sin, x)\n",
    "forward(::ScalarOperator{typeof(sin)}, x) = return sin(x)\n",
    "backward(::ScalarOperator{typeof(sin)}, x, g) = tuple(g * cos(x))\n",
    "import Base: exp\n",
    "exp(x::GraphNode) = ScalarOperator(exp, x)\n",
    "forward(::ScalarOperator{typeof(exp)}, x) = return exp(x)\n",
    "backward(::ScalarOperator{typeof(exp)}, x, g) = tuple(g * exp(x))\n",
    "import Base: max\n",
    "max(x::GraphNode) = ScalarOperator(max, x)\n",
    "forward(::ScalarOperator{typeof(max)}, x) = return max(x)\n",
    "backward(::ScalarOperator{typeof(max)}, x, g) = tuple(g * exp(x))\n",
    "\n",
    "\n",
    "import Base: *\n",
    "import LinearAlgebra: mul!, diagm\n",
    "# x * y (aka matrix multiplication)\n",
    "*(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)\n",
    "forward(::BroadcastedOperator{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::BroadcastedOperator{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "# x .* y (element-wise multiplication)pu\n",
    "Base.Broadcast.broadcasted(*, x::GraphNode, y::GraphNode) = BroadcastedOperator(*, x, y)\n",
    "forward(::BroadcastedOperator{typeof(*)}, x, y) = return x .* y\n",
    "backward(node::BroadcastedOperator{typeof(*)}, x, y, g) = let\n",
    "    ones_vec = ones(length(node.output)) # I wektor jednostkowy\n",
    "    Jx = diagm(y .* ones_vec) # I(length(node.output)) * yI\n",
    "    Jy = diagm(x .* ones_vec)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "Base.Broadcast.broadcasted(-, x::GraphNode) = BroadcastedOperator(-, x)\n",
    "forward(::BroadcastedOperator{typeof(-)}, x) = return .- x\n",
    "backward(::BroadcastedOperator{typeof(-)}, x, g) = tuple(g,-g)\n",
    "\n",
    "\n",
    "Base.Broadcast.broadcasted(-, x::GraphNode, y::GraphNode) = BroadcastedOperator(-, x, y)\n",
    "forward(::BroadcastedOperator{typeof(-)}, x, y) = return x .- y\n",
    "backward(::BroadcastedOperator{typeof(-)}, x, y, g) = tuple(g,-g)\n",
    "Base.Broadcast.broadcasted(+, x::GraphNode, y::GraphNode) = BroadcastedOperator(+, x, y)\n",
    "forward(::BroadcastedOperator{typeof(+)}, x, y) = return x .+ y\n",
    "backward(::BroadcastedOperator{typeof(+)}, x, y, g) = tuple(g, g)\n",
    "import Base: sum\n",
    "sum(x::GraphNode) = BroadcastedOperator(sum, x)\n",
    "forward(::BroadcastedOperator{typeof(sum)}, x) = return sum(x)\n",
    "backward(::BroadcastedOperator{typeof(sum)}, x, g) = let\n",
    "    ùüè = ones(length(x))\n",
    "    J = ùüè'\n",
    "    tuple(J' * g)\n",
    "end\n",
    "\n",
    "\n",
    "Base.Broadcast.broadcasted(/, x::GraphNode, y::GraphNode) = BroadcastedOperator(/, x, y)\n",
    "forward(::BroadcastedOperator{typeof(/)}, x, y) = return x ./ y\n",
    "backward(node::BroadcastedOperator{typeof(/)}, x, y, g) = let\n",
    "    ùüè = ones(length(node.output))\n",
    "    Jx = diagm(ùüè ./ y)\n",
    "    Jy = (-x ./ y .^2)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "import Base: max\n",
    "Base.Broadcast.broadcasted(max, x::GraphNode, y::GraphNode) = BroadcastedOperator(max, x, y)\n",
    "forward(::BroadcastedOperator{typeof(max)}, x, y) = return max.(x, y)\n",
    "backward(::BroadcastedOperator{typeof(max)}, x, y, g) = let\n",
    "    Jx = diagm(isless.(y, x))\n",
    "    Jy = diagm(isless.(x, y))\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "import Base: sum\n",
    "sum(x::GraphNode) = BroadcastedOperator(sum, x)\n",
    "forward(::BroadcastedOperator{typeof(sum)}, x) = return sum(x)\n",
    "backward(::BroadcastedOperator{typeof(sum)}, x, g) = let\n",
    "    ùüè = ones(length(x))\n",
    "    J = ùüè'\n",
    "    tuple(J' * g)\n",
    "end\n",
    "\n",
    "\n",
    "Base.Broadcast.broadcasted(max, x::GraphNode) = BroadcastedOperator(max, x)\n",
    "forward(::BroadcastedOperator{typeof(max)}, x) = return max.(x)\n",
    "backward(node::BroadcastedOperator{typeof(max)}, x, g) = let\n",
    "    index = argmax(node.output)\n",
    "    ùüè = zeros(length(x))\n",
    "    ùüè[index] = 1\n",
    "    J = ùüè'\n",
    "    tuple(J * g)\n",
    "end\n",
    "\n",
    "import Base: log\n",
    "Base.Broadcast.broadcasted(log, x::GraphNode) = BroadcastedOperator(log, x)\n",
    "forward(::BroadcastedOperator{typeof(log)}, x) = return log.(x)\n",
    "backward(node::BroadcastedOperator{typeof(log)}, x, g) = let\n",
    "    ùüè = ones(length(node.output))\n",
    "    J = diagm(ùüè ./ x)\n",
    "    tuple(J' * g)\n",
    "end\n",
    "\n",
    "\n",
    "Base.Broadcast.broadcasted(sin, x::GraphNode) = BroadcastedOperator(sin, x)\n",
    "forward(::BroadcastedOperator{typeof(sin)}, x) = return cos.(x)\n",
    "backward(::BroadcastedOperator{typeof(sin)}, x, g) = let\n",
    "    J = diagm(cos.(x))\n",
    "    tuple(J' * g)\n",
    "end\n",
    "\n",
    "Base.Broadcast.broadcasted(exp, x::GraphNode) = BroadcastedOperator(exp, x)\n",
    "forward(::BroadcastedOperator{typeof(exp)}, x) = return exp.(x)\n",
    "backward(::BroadcastedOperator{typeof(exp)}, x, g) = let\n",
    "    J = diagm(exp.(x))\n",
    "    tuple(J' * g)\n",
    "end\n",
    "\n",
    "\n",
    "œÉ(x::Real) = one(x) / (one(x) + exp(-x))\n",
    "œÉ(x::GraphNode) = BroadcastedOperator(œÉ, x)\n",
    "forward(::BroadcastedOperator{typeof(œÉ)}, x) = return 1.0 ./ (1.0 .+ exp.(-x))\n",
    "backward(::BroadcastedOperator{typeof(œÉ)}, x, g) = let\n",
    "    J = diagm(1.0 ./ (1.0 .+ exp.(-x))).*(1.0 .- (1.0 ./ (1.0 .+ exp.(-x))))\n",
    "    tuple(J' * g)\n",
    "end\n",
    "\n",
    "relu(x::Real) = max(zero(x), x)\n",
    "relu(x::GraphNode) = BroadcastedOperator(relu, x)\n",
    "forward(::BroadcastedOperator{typeof(relu)}, x) = return max.(zero(x), x)\n",
    "backward(::BroadcastedOperator{typeof(relu)}, x, g) = tuple(g .* (x .> 0))\n",
    "\n",
    "\n",
    "import Base: identity\n",
    "identity(x::GraphNode) = BroadcastedOperator(identity, x)\n",
    "forward(::BroadcastedOperator{typeof(identity)}, x) = return x\n",
    "backward(node::BroadcastedOperator{typeof(identity)}, x, g) = let\n",
    "    tuple(g)\n",
    "end\n",
    "\n",
    "Base.Broadcast.broadcasted(^, x::GraphNode, y::GraphNode) = BroadcastedOperator(^, x, y)\n",
    "forward(::BroadcastedOperator{typeof(^)}, x, y) = return x .^ y\n",
    "backward(node::BroadcastedOperator{typeof(^)}, x, y, g) = let\n",
    "    Jx = diagm(y .* x .^ (y .- 1.0))\n",
    "    Jy = diagm(log.(abs.(x)) .* x .^ y)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "import Base.reshape\n",
    "reshape(x::GraphNode, ndims::GraphNode) = BroadcastedOperator(reshape, x, ndims)\n",
    "forward(::BroadcastedOperator{typeof(reshape)}, x, ndims) = reshape(x, ndims)\n",
    "backward(::BroadcastedOperator{typeof(reshape)}, x, ndims, g) =\n",
    "    tuple(reshape(g, size(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M√≥j kod w≈Ça≈õciwa czƒô≈õƒá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_and_accuracy(net, test_data) = 8.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚îå Info: 1\n",
      "‚îÇ   acc = 86.58\n",
      "‚îÇ   test_acc = 87.39\n",
      "‚îî @ Main d:\\Studia\\Chabrystyka\\K2.ipynb:296\n",
      "‚îå Info: 2\n",
      "‚îÇ   acc = 90.82\n",
      "‚îÇ   test_acc = 91.28\n",
      "‚îî @ Main d:\\Studia\\Chabrystyka\\K2.ipynb:296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3374.921619 seconds (18.77 G allocations: 3.748 TiB, 11.18% gc time, 0.03% compilation time: 4% of which was recompilation)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚îå Info: 3\n",
      "‚îÇ   acc = 93.14\n",
      "‚îÇ   test_acc = 93.49\n",
      "‚îî @ Main d:\\Studia\\Chabrystyka\\K2.ipynb:296\n"
     ]
    }
   ],
   "source": [
    "logit_cross_entropy(y_predicted::GraphNode, y::GraphNode) = BroadcastedOperator(logit_cross_entropy, y_predicted, y)\n",
    "forward(::BroadcastedOperator{typeof(logit_cross_entropy)}, y_predicted, y) =\n",
    "let\n",
    "    y_shifted = y_predicted .- maximum(y_predicted)\n",
    "    shifted_logsumexp = log.(sum(exp.(y_shifted)))\n",
    "    result = y_shifted .- shifted_logsumexp\n",
    "    loss = -1 .* sum(y .* result)\n",
    "    return loss\n",
    "end\n",
    "backward(::BroadcastedOperator{typeof(logit_cross_entropy)}, y_predicted, y, g) =\n",
    "let\n",
    "    y_predicted = y_predicted .- maximum(y_predicted)\n",
    "    y_predicted = exp.(y_predicted) ./ sum(exp.(y_predicted))\n",
    "    result = (y_predicted - y)\n",
    "    return tuple(g .* result)\n",
    "end\n",
    "\n",
    "function dense(w, b, x, activation) \n",
    "    return activation((w * x) .+ b) \n",
    "end\n",
    "function dense(w, x, activation) return activation(w * x) end\n",
    "\n",
    "\n",
    "function flatten(x) return reshape(x, length(x)) end\n",
    "flatten(x::GraphNode) = BroadcastedOperator(flatten, x)\n",
    "forward(::BroadcastedOperator{typeof(flatten)}, x) = reshape(x, length(x))\n",
    "backward(::BroadcastedOperator{typeof(flatten)}, x, g) = tuple(reshape(g, size(x)))\n",
    "\n",
    "function maxPool(x, kernel_size)\n",
    "    N, C, H, W = size(x)\n",
    "    K_H = kernel_size[1]\n",
    "    K_W = kernel_size[2]\n",
    "    W_2 = fld(W - K_W, K_W) + 1\n",
    "    H_2 = fld(H - K_H ,K_H) + 1\n",
    "    if H_2 % 2 == 1\n",
    "        H -= 1\n",
    "        W -= 1\n",
    "    end\n",
    "    out = zeros(N, C, H_2, W_2)\n",
    "    for n=1:N\n",
    "        for c=1:C\n",
    "            for h=1:K_H:H\n",
    "                for w=1:K_W:W\n",
    "                    out[n, c, Int.((w+1)/K_W), Int.((h+1)/K_H)] = maximum(x[n, c, h:h+K_H-1,w:w+K_W-1])\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "\n",
    "function maxPoolB(x, g, kernel_size)\n",
    "    N, C, H, W = size(x)\n",
    "    Gn, Gc, Gh, Gw = size(g)\n",
    "    dx = zeros(N, C, H, W)\n",
    "    K_H = kernel_size[1]\n",
    "    K_W = kernel_size[2]\n",
    "    for n=1:Gn\n",
    "        for c=1:Gc\n",
    "            for h=1:Gh\n",
    "                for w=1:Gw\n",
    "                    max_val = x[n, c, 1+(h-1)*K_H:h*(K_H), 1+(w-1)*K_W:w*K_W]\n",
    "                    max_h, max_w = Tuple.(findmax(max_val)[2])\n",
    "                    max_h += (h-1)*K_H\n",
    "                    max_w += (w-1)*K_W\n",
    "                    dx[n,c,max_h,max_w] = g[n,c,h,w]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return tuple(dx)\n",
    "end\n",
    "\n",
    "maxPool(x::GraphNode, kernel_size:: Any) = BroadcastedOperator(maxPool, x, kernel_size)\n",
    "forward(::BroadcastedOperator{typeof(maxPool)}, x, kernel_size) = maxPool(x, kernel_size)\n",
    "backward(::BroadcastedOperator{typeof(maxPool)}, x, kernel_size, g) = maxPoolB(x, g, kernel_size)\n",
    "\n",
    "\n",
    "function conv(I, K, b)\n",
    "    N, C, H, W = size(I)\n",
    "    F, C, HH, WW = size(K)\n",
    "    H_R = 1 + H - HH\n",
    "    W_R = 1 + W - WW\n",
    "    out = zeros(N, F, H_R, W_R)\n",
    "    for n=1:N\n",
    "        for depth=1:F\n",
    "            for r=1:H_R\n",
    "                for c=1:W_R\n",
    "                    out[n, depth, r, c] = sum(I[n,:, r:r+HH-1, c:c+WW-1] .* K[depth, :, :, :]) + b[depth]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "conv(x::GraphNode, w::GraphNode, b::GraphNode) = BroadcastedOperator(conv, x, w, b)\n",
    "forward(::BroadcastedOperator{typeof(conv)}, x, w, b) = conv(x, w, b)\n",
    "backward(::BroadcastedOperator{typeof(conv)}, x, w, b, g) = let \n",
    "    N, F, H_R, W_R = size(g)\n",
    "    N, C, H, W = size(x)\n",
    "    F, C, HH, WW = size(w)\n",
    "    dx = zeros(size(x))\n",
    "    dw = zeros(size(w))\n",
    "    db = zeros(size(b))\n",
    "    for n=1:N\n",
    "        for depth=1:F\n",
    "            for r=1:H_R\n",
    "                for c=1:W_R\n",
    "                    wu = w[depth, :, :, :]\n",
    "                    gje = g[n, depth, r, c]\n",
    "                    dx[n, :, r:r+HH-1, c:c+WW-1] += wu .* gje\n",
    "                    dw[depth, :, :, :] += x[n, :, r:r+HH-1, c:c+WW-1] .* g[n, depth, r, c]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    for depth=1:F\n",
    "        db[depth] = sum(g[:, depth, :, :])\n",
    "    end\n",
    "    return tuple(dx, dw, db)\n",
    "end\n",
    "\n",
    "function create_kernels(n_input, n_output, kernel_width, kernel_height)\n",
    "    # Inicjalizacja Xaviera\n",
    "    squid = sqrt(6 / (n_input + n_output * (kernel_width * kernel_height)))\n",
    "    random_vals = randn(n_output, n_input, kernel_width, kernel_height) * squid\n",
    "    return Variable(random_vals)\n",
    "end\n",
    "\n",
    "function xavier_init(n_input, n_output)\n",
    "    return Variable(randn(n_input, n_output) * sqrt(6 / (n_input + n_output)))\n",
    "end\n",
    "\n",
    "# ≈ªebym m√≥g≈Ç ≈Çadnie sieƒá zdefiniowaƒá\n",
    "abstract type NetworkLayer end\n",
    "mutable struct Network\n",
    "    layers\n",
    "end\n",
    "function Network(layers...)\n",
    "    return Network(layers)\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct aDense <: NetworkLayer\n",
    "    weights :: Variable\n",
    "    bias :: Variable\n",
    "    activation :: Function\n",
    "    func :: Function\n",
    "    aDense(pair, activation) = new(xavier_init(pair[2], pair[1]), Variable(zeros(pair[2])), activation, dense)\n",
    "end\n",
    "\n",
    "mutable struct aMaxPool <: NetworkLayer\n",
    "    kernel_size :: Any\n",
    "    func :: Function\n",
    "    aMaxPool(kernel_size) = new(Constant(kernel_size), maxPool)\n",
    "end\n",
    "\n",
    "mutable struct aFlatten <: NetworkLayer\n",
    "    func :: Function\n",
    "    aFlatten() = new(flatten)\n",
    "end\n",
    "\n",
    "mutable struct aConv2d <: NetworkLayer\n",
    "    weights :: Variable\n",
    "    bias :: Variable\n",
    "    activation :: Function\n",
    "    func :: Function\n",
    "end\n",
    "aConv(filter_size, pair, activation) = aConv2d(create_kernels(pair[1], pair[2], filter_size[1], filter_size[2]), Variable(zeros(pair[2])), activation, conv)\n",
    "\n",
    "\n",
    "net = Network(\n",
    "    aConv((3, 3), 1 => 6, relu),\n",
    "    aMaxPool((2,2)),\n",
    "    aConv((3, 3), 6 => 16, relu),\n",
    "    aMaxPool((2,2)),\n",
    "    aFlatten(),\n",
    "    aDense(400 => 84, relu),\n",
    "    aDense(84 => 10, identity)\n",
    ")\n",
    "\n",
    "(n::Network)(x) = begin\n",
    "    for layer in n.layers\n",
    "        if layer.func == dense\n",
    "            x = layer.func(layer.weights, layer.bias, x, layer.activation)\n",
    "        elseif layer.func == conv\n",
    "            x = layer.func(x, layer.weights, layer.bias)\n",
    "            x = layer.activation(x)\n",
    "        elseif layer.func == maxPool\n",
    "             x = layer.func(x, layer.kernel_size)\n",
    "        else\n",
    "            x = layer.func(x)\n",
    "        end\n",
    "    end\n",
    "    return argmax(forward!(topological_sort(x)))-1\n",
    "end\n",
    "\n",
    "create_graph(n::Network, x) = begin\n",
    "    for layer in n.layers\n",
    "        if layer.func == dense\n",
    "            x = layer.func(layer.weights, layer.bias, x, layer.activation)\n",
    "        elseif layer.func == conv\n",
    "            x = layer.func(x, layer.weights, layer.bias)\n",
    "            x = layer.activation(x)\n",
    "        elseif layer.func == maxPool\n",
    "             x = layer.func(x, layer.kernel_size)\n",
    "        else\n",
    "            x = layer.func(x)\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "agrad(loss_func, y_predicted, y_true) = begin\n",
    "    loss = loss_func(y_predicted, y_true)\n",
    "    order = topological_sort(loss)\n",
    "    return order\n",
    "end\n",
    "\n",
    "function update_weights!(n::Network, batchsize)\n",
    "    for layer in n.layers\n",
    "        if layer.func == dense\n",
    "            layer.weights.output .-= layer.weights.cache / batchsize\n",
    "            layer.bias.output .-= layer.bias.cache / batchsize\n",
    "            layer.bias.cache .= 0\n",
    "            layer.weights.cache .= 0\n",
    "        elseif layer.func == conv\n",
    "            layer.weights.output .-= layer.weights.cache / batchsize\n",
    "            layer.bias.output .-= layer.bias.cache / batchsize\n",
    "            layer.bias.cache .= 0\n",
    "            layer.weights.cache .= 0\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function update_cache!(n::Network, order, learning_rate)\n",
    "    backward!(order)\n",
    "    for layer in n.layers\n",
    "        if layer.func == dense\n",
    "            layer.weights.cache .+= learning_rate .* layer.weights.gradient\n",
    "            layer.bias.cache .+= learning_rate .* layer.bias.gradient\n",
    "        elseif layer.func == conv\n",
    "            layer.weights.cache .+= learning_rate .* layer.weights.gradient\n",
    "            layer.bias.cache .+= learning_rate .* layer.bias.gradient\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "settings = (;\n",
    "    eta = 1e-2,\n",
    "    epochs = 3,\n",
    "    batchsize = 100,\n",
    ")\n",
    "\n",
    "function loss_and_accuracy(model, data)\n",
    "    x_e = data.features\n",
    "    yhot_e = Flux.onehotbatch(data.targets, 0:9)\n",
    "    size = length(data)\n",
    "    suma = size\n",
    "    poprawne = 0\n",
    "    for i=1:size\n",
    "        x = Variable(reshape(x_e[:, :, i], 1, 1, 28, 28), name=\"x\") \n",
    "        y = yhot_e[:, i]\n",
    "        result = model(x)\n",
    "        if result == argmax(y)-1\n",
    "            poprawne += 1\n",
    "        end\n",
    "    end\n",
    "    acc = round(100 * poprawne/suma; digits=2)\n",
    "    return acc\n",
    "end\n",
    "\n",
    "@show loss_and_accuracy(net, test_data);  # accuracy about 10%, before training\n",
    "\n",
    "\n",
    "@time for epoch=1:settings.epochs\n",
    "    batchsize = settings.batchsize\n",
    "    batch_counter = 0\n",
    "    for i=1:size(train_data.features)[3]\n",
    "        batch_counter += 1\n",
    "        x = Variable(reshape(x1[:, :, i], 1, 1, 28, 28), name=\"x\")\n",
    "        y1 = Variable(yhot[:, i], name=\"y\")\n",
    "        graph = create_graph(net, x)\n",
    "        full = agrad(logit_cross_entropy, graph, y1)\n",
    "        forward!(full)\n",
    "        update_cache!(net, full, settings.eta)\n",
    "        if batch_counter == batchsize\n",
    "            update_weights!(net, batchsize)\n",
    "            batch_counter = 0\n",
    "        end\n",
    "    end\n",
    "    acc = loss_and_accuracy(net, train_data)\n",
    "    test_acc = loss_and_accuracy(net, test_data)\n",
    "    @info epoch acc test_acc\n",
    "end  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynki:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
