{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets, Flux, Statistics\n",
    "train_data = MLDatasets.MNIST(split=:train)\n",
    "test_data  = MLDatasets.MNIST(split=:test)\n",
    "\n",
    "\n",
    "function loader(data)\n",
    "    dim1, dim2, dim3 = size(data.features)\n",
    "    x = reshape(data.features, dim1 * dim2, dim3)\n",
    "    y = data.targets\n",
    "    #x4dim = reshape(data.features, 28, 28, 1, :) # insert trivial channel dim\n",
    "    yhot  = Flux.onehotbatch(data.targets, 0:9)  # make a 10×60000 OneHotMatrix\n",
    "    return x, y, yhot\n",
    "    #Flux.DataLoader((x4dim, yhot); batchsize, shuffle=true)\n",
    "end\n",
    "\n",
    "x1, y1, yhot = loader(train_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hessian (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "struct Dual{T <:Number} <:Number\n",
    "    v::T\n",
    "   dv::T\n",
    "end\n",
    "# Przeciążenie podstawowych operatorów\n",
    "import Base: +, -, *, /\n",
    "-(x::Dual)          = Dual(-x.v,       -x.dv)\n",
    "+(x::Dual, y::Dual) = Dual( x.v + y.v,  x.dv + y.dv)\n",
    "-(x::Dual, y::Dual) = Dual( x.v - y.v,  x.dv - y.dv)\n",
    "*(x::Dual, y::Dual) = Dual( x.v * y.v,  x.dv * y.v + x.v * y.dv)\n",
    "/(x::Dual, y::Dual) = Dual( x.v / y.v, (x.dv * y.v - x.v * y.dv)/y.v^2)\n",
    "# Przeciążenie podstawowych funkcji\n",
    "import Base: abs, sin, cos, tan, exp, sqrt, isless, log\n",
    "abs(x::Dual)  = Dual(abs(x.v),sign(x.v)*x.dv)\n",
    "sin(x::Dual)  = Dual(sin(x.v), cos(x.v)*x.dv)\n",
    "cos(x::Dual)  = Dual(cos(x.v),-sin(x.v)*x.dv)\n",
    "tan(x::Dual)  = Dual(tan(x.v), one(x.v)*x.dv + tan(x.v)^2*x.dv)\n",
    "exp(x::Dual)  = Dual(exp(x.v), exp(x.v)*x.dv)\n",
    "sqrt(x::Dual) = Dual(sqrt(x.v),.5/sqrt(x.v) * x.dv)\n",
    "log(x::Dual)  = Dual(log(x.v), x.dv/x.v)\n",
    "isless(x::Dual, y::Dual) = x.v < y.v;\n",
    "# Promocja typów i konwersja\n",
    "import Base: convert, promote_rule\n",
    "convert(::Type{Dual{T}}, x::Dual) where T = Dual(convert(T, x.v), convert(T, x.dv))\n",
    "convert(::Type{Dual{T}}, x::Number) where T = Dual(convert(T, x), zero(T))\n",
    "promote_rule(::Type{Dual{T}}, ::Type{R}) where {T,R} = Dual{promote_type(T,R)}\n",
    "# Pomocne funkcje\n",
    "import Base: show\n",
    "show(io::IO, x::Dual) = print(io, \"(\", x.v, \") + [\", x.dv, \"ϵ]\");\n",
    "value(x::Dual) = x.v;\n",
    "partials(x::Dual) = x.dv;\n",
    "ReLU(x) = max(zero(x), x)\n",
    "σ(x) = one(x) / (one(x) + exp(-x))\n",
    "tanh(x) = 2.0 / (one(x) + exp(-2.0x)) - one(x)\n",
    "ϵ = Dual(0., 1.)\n",
    "D = derivative(f, x) = partials(f(Dual(x, one(x))))\n",
    "J = function jacobian(f, args::Vector{T}) where {T <:Number}\n",
    "    jacobian_columns = Matrix{T}[]\n",
    "    \n",
    "    for i=1:length(args)\n",
    "        x = Dual{T}[]\n",
    "        for j=1:length(args)\n",
    "            if i == j\n",
    "                push!(x, Dual(args[j], one(args[j])))\n",
    "            else\n",
    "                push!(x, Dual(args[j], zero(args[j])))\n",
    "            end\n",
    "        end\n",
    "        column = partials.([f(x)...])\n",
    "        push!(jacobian_columns, column[:,:])\n",
    "    end\n",
    "    hcat(jacobian_columns...)\n",
    "end\n",
    "\n",
    "H = function hessian(f, args::Vector)\n",
    "    ∇f(x::Vector) = J(f, x)\n",
    "    J(∇f, args)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logit_cross_entropy (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function log_softmax(x)\n",
    "    # Subtract the max value for numerical stability\n",
    "    shift_x = x .- max.(x)\n",
    "    log_exp_sum = log.(sum(exp.(shift_x)))\n",
    "    res = shift_x .- log_exp_sum\n",
    "    println(res)\n",
    "    return res\n",
    "end\n",
    "\n",
    "\n",
    "function logit_cross_entropy(y_pred, y_true)\n",
    "    return (-sum(y_true .* log_softmax(y_pred)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.039678743362121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = Vector{Bool}([0, 0, 0, 0, 0, 1, 0, 0, 0, 0]);\n",
    "Flux.logitcrossentropy([1.9514772580227577, 1.848623792879922, -6.456514365666464, -5.10149420660439, 3.5482988599977925, -2.210665982825723, 4.338133361390976, -8.256227686203022, -9.262014936475973, -1.3432975629321589], a)\n",
    "#logit_cross_entropy([1.9514772580227577, 1.848623792879922, -6.456514365666464, -5.10149420660439, 3.5482988599977925, -2.210665982825723, 4.338133361390976, -8.256227686203022, -9.262014936475973, -1.3432975629321589], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "net (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_squared_loss(y::Vector, ŷ::Vector) = sum(0.5(y - ŷ).^2)\n",
    "fullyconnected(w::Vector, n::Number, m::Number, v::Vector, activation::Function) = activation.(reshape(w, n, m) * v)\n",
    "n1 = 25\n",
    "input = 784\n",
    "output = 10\n",
    "Wh  = randn(n1,input) # 50 x 784\n",
    "Wo  = randn(output,n1) # 10 x 50\n",
    "dWh = similar(Wh)\n",
    "dWo = similar(Wo)\n",
    "E = Float64[]\n",
    "correct = 0\n",
    "all = 0\n",
    "x = x1[:, 1]\n",
    "y = yhot[:,1]\n",
    "#yhot = yhot[:,1]\n",
    "\n",
    "function net(x, wh, wo, y)\n",
    "    x̂ = fullyconnected(wh, n1, input, x, σ)\n",
    "    ŷ = fullyconnected(wo, output, n1, x̂, u -> u)\n",
    "    E = logit_cross_entropy(ŷ, y)\n",
    "end\n",
    "\n",
    "#Ei = net(x, Wh[:],  Wo[:], y, yhot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dupa1\n",
      "Loss:0.6528753311036898\n",
      "Loss:6.9603802176279785\n",
      "Loss:3.931078524913497\n",
      "Loss:0.3094221998815187\n",
      "Loss:2.3856341460429835\n",
      "Loss:1.2105866231086957\n",
      "Loss:1.2719136335020222\n",
      "Loss:3.0105453997267086\n",
      "Loss:0.6001055288727974\n",
      "Loss:9.209726832173196\n",
      "Loss:2.7539565848165646\n",
      "Loss:4.863962249562979\n",
      "Loss:1.6359622113903183\n",
      "Loss:0.9143057111944424\n",
      "Loss:0.0803250000828219\n",
      "Loss:1.477061033036103\n",
      "Loss:2.1660688319227126\n",
      "Loss:5.150468581081785\n",
      "Loss:6.246379599232945\n",
      "Loss:2.780069746095853\n",
      "Loss:0.3029446323574035\n",
      "Loss:2.7545059606549893\n",
      "Loss:0.7342529798738292\n",
      "Loss:1.947271219407915\n",
      "Loss:2.2430138586774744\n",
      "Loss:1.017403732307133\n",
      "Loss:4.0675860717720855\n",
      "Loss:2.887866597855485\n",
      "Loss:4.0079472511311725\n",
      "Loss:0.429221664794923\n",
      "Loss:0.8329438054345466\n",
      "Loss:1.0987673200713433\n",
      "Loss:1.3620602028745799\n",
      "Loss:6.82186860378852\n",
      "Loss:1.7521038147708012\n",
      "Loss:1.7932292193331467\n",
      "Loss:3.3643772866778736\n",
      "Loss:0.95134005399703\n",
      "Loss:3.5781109406228526\n",
      "Loss:4.270840968103776\n",
      "Loss:1.2101079236229881\n",
      "Loss:3.884226173632929\n",
      "Loss:1.0319244481654128\n",
      "Loss:2.0494007005134547\n",
      "Loss:5.385368534263936\n",
      "Loss:4.13509541830213\n",
      "Loss:6.064155114615181\n",
      "Loss:3.7640040810728723\n",
      "Loss:2.877760836639001\n",
      "Loss:3.174351470389496\n",
      "Loss:0.9673459593443324\n",
      "Loss:0.9004744639996196\n",
      "Loss:0.27921894779203327\n",
      "Loss:0.7887416972820045\n",
      "Loss:2.228570750447034\n",
      "Loss:3.1327629471311678\n",
      "Loss:0.21461670149129583\n",
      "Loss:3.2402033359253473\n",
      "Loss:4.11537563083381\n",
      "Loss:4.986245106221689\n",
      "Loss:4.989809370544179\n",
      "Loss:3.1843876438500462\n",
      "Loss:3.5527550668207835\n",
      "Loss:2.3109426022078114\n",
      "Loss:0.6851943428359419\n",
      "Loss:0.4076631092305246\n",
      "Loss:0.8845782125230192\n",
      "Loss:1.9853877724249984\n",
      "Loss:3.6959342435026485\n",
      "Loss:3.7329027889440978\n",
      "Loss:0.34887690499769863\n",
      "Loss:0.9195200284349506\n",
      "Loss:2.294475723218028\n",
      "Loss:1.3931744109307351\n",
      "Loss:6.7619688833381595\n",
      "Loss:0.48276329808466756\n",
      "Loss:5.130079931784165\n",
      "Loss:1.4582568605475117\n",
      "Loss:2.1349572751465167\n",
      "Loss:5.668163458847375\n",
      "Loss:3.3210830192907466\n",
      "Loss:0.38423986030987306\n",
      "Loss:0.6154457583650199\n",
      "Loss:3.631050417464631\n",
      "Loss:0.5149730041910177\n",
      "Loss:0.25766579113250426\n",
      "Loss:1.953661720047531\n",
      "Loss:5.095288749591542\n",
      "Loss:0.4234820305684592\n",
      "Loss:3.3058373333270508\n",
      "Loss:0.738862597616636\n",
      "Loss:0.2120716026557849\n",
      "Loss:4.49089741004554\n",
      "Loss:0.6574679306817471\n",
      "Loss:0.8289124218841596\n",
      "Loss:3.913698630296479\n",
      "Loss:2.485428330750277\n",
      "Loss:3.540949328561302\n",
      "Loss:0.7798588552218352\n",
      "Loss:1.8496202151113295\n",
      "Loss:2.481610421428578\n",
      "Loss:1.284604853565457\n",
      "Loss:0.7926886343056423\n",
      "Loss:0.7523435559420403\n",
      "Loss:0.42528604667536374\n",
      "Loss:4.995292100549925\n",
      "Loss:0.20675196211616925\n",
      "Loss:2.5691500944938017\n",
      "Loss:2.3269478682636406\n",
      "Loss:0.36262113109691746\n",
      "Loss:0.7733247418698255\n",
      "Loss:0.36793999314106\n",
      "Loss:0.6895058644817853\n",
      "Loss:3.0402906896125286\n",
      "Loss:1.4673822849037659\n",
      "Loss:0.8506707086322982\n",
      "Loss:2.333765595063247\n",
      "Loss:0.5779042328043705\n",
      "Loss:3.4627112463902545\n",
      "Loss:2.8931517595380347\n",
      "Loss:6.353432278182609\n",
      "Loss:4.029608789132521\n",
      "Loss:2.0015177072499926\n",
      "Loss:2.2359163479291326\n",
      "Loss:1.1109721301264242\n",
      "Loss:3.4274257211137096\n",
      "Loss:3.4918365275424232\n",
      "Loss:4.385150818397289\n",
      "Loss:0.6452398664948058\n",
      "Loss:2.118131835975615\n",
      "Loss:1.0026518685264603\n",
      "Loss:1.8368975487337862\n",
      "Loss:0.8179870257749414\n",
      "Loss:0.28865606111737513\n",
      "Loss:2.5223518174104127\n",
      "Loss:0.13189287686917045\n",
      "Loss:0.4848781020676288\n",
      "Loss:5.45603765192008\n",
      "Loss:0.9079619133451817\n",
      "Loss:0.7473267689058489\n",
      "Loss:3.9442736652316546\n",
      "Loss:5.717458345621802\n",
      "Loss:1.377408603961438\n",
      "Loss:0.5528525922960938\n",
      "Loss:1.4726843738818765\n",
      "Loss:1.6428574060942749\n",
      "Loss:1.3904897515783488\n",
      "Loss:3.2737333066428995\n",
      "Loss:0.8678403045063959\n",
      "Loss:0.1503897073876713\n",
      "Loss:0.5589443123866369\n",
      "Loss:1.1157870838438526\n",
      "Loss:0.7107306875108554\n",
      "Loss:3.087887168240851\n",
      "Loss:1.2993572800527704\n",
      "Loss:4.2990693696563556\n",
      "Loss:1.3019650264795675\n",
      "Loss:2.8810974379005607\n",
      "Loss:0.8481882681329694\n",
      "Loss:2.159335399619179\n",
      "Loss:4.0713209904897205\n",
      "Loss:3.2487631777490336\n",
      "Loss:0.12020623951038759\n",
      "Loss:4.050213794893428\n",
      "Loss:0.7642913055010854\n",
      "Loss:4.810771119761755\n",
      "Loss:0.5523895831798684\n",
      "Loss:3.8199316289109864\n",
      "Loss:1.0082764567701719\n",
      "Loss:1.2292048498571524\n",
      "Loss:0.8448547454908715\n",
      "Loss:0.5630218963452752\n",
      "Loss:0.5247664797173995\n",
      "Loss:2.4002807947841376\n",
      "Loss:2.486287790817244\n",
      "Loss:0.44271841786405886\n",
      "Loss:0.5353963423009723\n",
      "Loss:0.1673141377484174\n",
      "Loss:0.9709527797275752\n",
      "Loss:3.071735986682728\n",
      "Loss:3.5275608268558454\n",
      "Loss:3.8449482800972112\n",
      "Loss:0.1600461273625771\n",
      "Loss:2.4505257205664597\n",
      "Loss:0.4328928607300646\n",
      "Loss:1.4839195442784894\n",
      "Loss:0.2398743709151756\n",
      "Loss:0.6844227499601115\n",
      "Loss:2.986500804297844\n",
      "Loss:4.824312481947684\n",
      "Loss:0.675241462282165\n",
      "Loss:0.5076412365703948\n",
      "Loss:0.8494751662943523\n",
      "Loss:2.137403119826942\n",
      "Loss:0.18763044284289768\n",
      "Loss:0.39316331323589415\n",
      "Loss:0.6156980781511475\n",
      "Loss:0.2809543094623972\n",
      "Loss:0.11653282405542674\n",
      "Loss:0.6689132797016436\n",
      "Loss:2.988741055930371\n",
      "Loss:3.0982737571260115\n",
      "Loss:0.9656743865988662\n",
      "Loss:0.14108044098786995\n",
      "Loss:0.07781144895456957\n",
      "Loss:2.4641220979859773\n",
      "Loss:0.5750761835903264\n",
      "Loss:0.9759239587196454\n",
      "Loss:0.6303831644203027\n",
      "Loss:1.9414346880880196\n",
      "Loss:0.647932752881323\n",
      "Loss:0.05746277723587838\n",
      "Loss:0.09101776132869036\n",
      "Loss:2.782674822174245\n",
      "Loss:1.1669717363686405\n",
      "Loss:0.8069248033062255\n",
      "Loss:0.3821532015773824\n",
      "Loss:0.744143650870846\n",
      "Loss:0.953044264637509\n",
      "Loss:1.0665175874869322\n",
      "Loss:2.040788205602579\n",
      "Loss:0.33683506641605754\n",
      "Loss:0.3557840263744825\n",
      "Loss:3.5938792739416927\n",
      "Loss:2.190478087651291\n",
      "Loss:0.6636481035969432\n",
      "Loss:1.0751903004082792\n",
      "Loss:0.643445171144402\n",
      "Loss:1.2050854377771933\n",
      "Loss:2.599207158609602\n",
      "Loss:0.30678849453682117\n",
      "Loss:0.9848069343776588\n",
      "Loss:0.384418948237273\n",
      "Loss:1.4937981703690333\n",
      "Loss:1.664904561674228\n",
      "Loss:1.635828348612676\n",
      "Loss:2.353449833307099\n",
      "Loss:0.6547707080441393\n",
      "Loss:1.993181945218272\n",
      "Loss:1.4002822967138329\n",
      "Loss:0.3803452426700567\n",
      "Loss:2.874002759006789\n",
      "Loss:0.30495448005581094\n",
      "Loss:0.8348920977738756\n",
      "Loss:0.761702622277986\n",
      "Loss:1.7321734412517777\n",
      "Loss:0.30568328160142183\n",
      "Loss:1.4884728352891443\n",
      "Loss:1.102903625964982\n",
      "Loss:1.6266086437339577\n",
      "Loss:0.9630280002130754\n",
      "Loss:1.2707455946418946\n",
      "Loss:0.4639028439068732\n",
      "Loss:3.6094442702382428\n",
      "Loss:0.8463178596120471\n",
      "Loss:2.0621064115621484\n",
      "Loss:0.4894806162645709\n",
      "Loss:0.6664830279634293\n",
      "Loss:0.14290340441688806\n",
      "Loss:0.3538438765474371\n",
      "Loss:0.17662988677884628\n",
      "Loss:1.5626819860154644\n",
      "Loss:1.0966495631280186\n",
      "Loss:1.7075177745427736\n",
      "Loss:0.2516403315888552\n",
      "Loss:0.8223508066585117\n",
      "Loss:0.3152195764117299\n",
      "Loss:0.3941606474986692\n",
      "Loss:0.7161883295508709\n",
      "Loss:3.5997266458129964\n",
      "Loss:0.8929378726630453\n",
      "Loss:5.852618998081729\n",
      "Loss:0.18126750203884207\n",
      "Loss:0.015127137197500326\n",
      "Loss:0.11019091669839298\n",
      "Loss:1.4924905336382825\n",
      "Loss:0.8671325424665774\n",
      "Loss:2.467853232793412\n",
      "Loss:0.9728925027447574\n",
      "Loss:0.2723297697194633\n",
      "Loss:1.5532695561021366\n",
      "Loss:0.588317581928675\n",
      "Loss:0.43109116903114225\n",
      "Loss:2.6269117562513906\n",
      "Loss:0.4286845791014079\n",
      "Loss:0.793226382467043\n",
      "Loss:0.17124899195371066\n",
      "Loss:3.483051770172714\n",
      "Loss:0.5312921983536647\n",
      "Loss:4.038728606770786\n",
      "Loss:0.6180910236385465\n",
      "Loss:1.129142238926817\n",
      "Loss:0.9459942654674451\n",
      "Loss:1.7793710284066915\n",
      "Loss:0.051992928041678335\n",
      "Loss:2.1318289381654014\n",
      "Loss:0.2784646480125704\n",
      "Loss:2.7441799129975037\n",
      "Loss:0.11376389431282573\n",
      "Loss:0.1100244532016784"
     ]
    }
   ],
   "source": [
    "dnet_Wh(x, wh, wo, y) = J(w -> net(x, w, wo, y), wh);\n",
    "dnet_Wo(x, wh, wo, y) = J(w -> net(x, wh, w, y), wo);\n",
    "println(\"dupa1\")\n",
    "for i=101:1000\n",
    "    Ei  = net(x, Wh[:], Wo[:], y)\n",
    "    println(\"Loss:\", Ei)\n",
    "    push!(E, Ei)\n",
    "    dWh[:] = dnet_Wh(x, Wh[:],  Wo[:], y);\n",
    "    dWo[:] = dnet_Wo(x, Wh[:], Wo[:], y);\n",
    "    Wh .-= 0.1dWh\n",
    "    Wo .-= 0.1dWo\n",
    "    x = x1[:, i]\n",
    "    y = yhot[:,i]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.060088640808289"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Flux.logitcrossentropy(Ei, yhot[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
